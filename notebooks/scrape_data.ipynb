{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load story metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 107/150 [01:34<00:39,  1.09it/s]/Users/chandan/Library/Python/3.7/lib/python/site-packages/bs4/__init__.py:177: UserWarning: You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\n",
      "  warnings.warn(\"You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\")\n",
      " 72%|███████▏  | 108/150 [01:35<00:35,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped after 107 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████▎  | 109/150 [01:36<00:30,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped after 108 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████▎  | 110/150 [01:36<00:28,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped after 109 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|███████▍  | 111/150 [01:37<00:25,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped after 110 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▍  | 112/150 [01:37<00:24,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped after 111 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 113/150 [01:38<00:24,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped after 112 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████▌  | 114/150 [01:39<00:27,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped after 113 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████▋  | 115/150 [01:40<00:26,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped after 114 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████▋  | 116/150 [01:41<00:27,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped after 115 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|███████▊  | 117/150 [01:42<00:28,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped after 116 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████▊  | 118/150 [01:42<00:25,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped after 117 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████▉  | 119/150 [01:43<00:23,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped after 118 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 120/150 [01:44<00:21,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped after 119 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 122/150 [01:44<00:12,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped after 120 pages\n",
      "stopped after 121 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 124/150 [01:44<00:07,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped after 122 pages\n",
      "stopped after 123 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 126/150 [01:45<00:05,  4.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped after 124 pages\n",
      "stopped after 125 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 128/150 [01:45<00:04,  5.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped after 126 pages\n",
      "stopped after 127 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 130/150 [01:45<00:03,  5.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped after 128 pages\n",
      "stopped after 129 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 132/150 [01:46<00:02,  6.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped after 130 pages\n",
      "stopped after 131 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|████████▊ | 133/150 [01:46<00:02,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped after 132 pages\n",
      "stopped after 133 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 136/150 [01:46<00:02,  6.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped after 134 pages\n",
      "stopped after 135 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 138/150 [01:46<00:01,  6.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped after 136 pages\n",
      "stopped after 137 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 140/150 [01:47<00:01,  6.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped after 138 pages\n",
      "stopped after 139 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 142/150 [01:47<00:01,  6.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped after 140 pages\n",
      "stopped after 141 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 144/150 [01:48<00:01,  5.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped after 142 pages\n",
      "stopped after 143 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 146/150 [01:48<00:00,  5.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped after 144 pages\n",
      "stopped after 145 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 148/150 [01:48<00:00,  6.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped after 146 pages\n",
      "stopped after 147 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|█████████▉| 149/150 [01:48<00:00,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped after 148 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 150/150 [01:49<00:00,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped after 149 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "url_root = 'https://www.allsides.com/'\n",
    "url_page = url_root + 'story/admin?page='\n",
    "tab_num = 0\n",
    "\n",
    "# loop over pages\n",
    "# there are about 100 pages as of 12/29/2020, so we set this number to be well above that\n",
    "dfs = []\n",
    "for tab_num in tqdm(range(150)):\n",
    "    # read in the source\n",
    "    try:\n",
    "        source = urllib.request.urlopen(url_page + str(tab_num))\n",
    "        sp = bs.BeautifulSoup(source, 'lxml')\n",
    "        table = sp.table\n",
    "        df = pd.read_html(str(table), encoding='utf-8', header=0)[0] # read table with no links\n",
    "\n",
    "        # get links\n",
    "        links = []\n",
    "        for tag in table.find_all('a'):\n",
    "            if tag.has_attr('href'):\n",
    "                link = tag.get('href')\n",
    "                if '/story' in link:\n",
    "                    links.append(link)\n",
    "            else:\n",
    "                print(f'error! missing a link for {link}')\n",
    "        df['url_story'] = links\n",
    "        dfs.append(df)\n",
    "    except:\n",
    "        print(f'stopped after {tab_num} pages')\n",
    "        break\n",
    "        \n",
    "df = pd.concat(dfs)\n",
    "df.to_pickle('../data/df_links.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5315/5315 [1:34:55<00:00,  1.02s/it]  \n"
     ]
    }
   ],
   "source": [
    "# add info from link\n",
    "def get_info_from_url_story(url_story):\n",
    "    '''add info rom url on a a story\n",
    "    '''\n",
    "    story = urllib.request.urlopen(url_story)\n",
    "    sp_story = bs.BeautifulSoup(story, 'html.parser')\n",
    "\n",
    "    \n",
    "    # extract info from specific story page\n",
    "    story_triplet_info = {}\n",
    "    \n",
    "    # loop over left, center, and right stories\n",
    "    try:\n",
    "        for div in sp_story.find_all('div', {'class': 'news-title'})[:3]:\n",
    "            title = div.a.contents[0]\n",
    "            url = div.a.get('href')\n",
    "\n",
    "            news_source = div.parent.find_all('div', {'class': 'news-source'})[0].contents[1]\n",
    "            leaning = div.parent.find_all('div', {'class': 'bias-image'})[0].img.get('title').replace(\"Political News Media Bias Rating: \", '')\n",
    "\n",
    "            news_text = '\\n'.join([s.contents[0] for s in \n",
    "                      div.parent.find_all('div', {'class': 'news-body'})[0].contents\n",
    "                      if 'Tag' in str(type(s))])\n",
    "            prefix = leaning.lower().replace('lean ', '') + '_story_'\n",
    "            story_info = {\n",
    "                f'{prefix}title': title,\n",
    "                f'{prefix}url': url,\n",
    "                f'{prefix}source': news_source,\n",
    "                f'{prefix}leaning': leaning,\n",
    "                f'{prefix}text': news_text\n",
    "            }\n",
    "            story_triplet_info = {**story_triplet_info, **story_info}\n",
    "    except:\n",
    "        return {}\n",
    "    return story_triplet_info\n",
    "\n",
    "def get_stories(df):\n",
    "    '''Add list for all stories\n",
    "    '''\n",
    "    story_triplet_list = []\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        url_story = url_root + df.iloc[i]['url_story']\n",
    "        story_triplet_info = get_info_from_url_story(url_story)\n",
    "        story_triplet_list.append(story_triplet_info)\n",
    "    return pd.DataFrame.from_dict(story_triplet_list)\n",
    "    \n",
    "df_stories = get_stories(df)\n",
    "\n",
    "# return df.join(df_stories).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while getting the str of an object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ca62a64a0836>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_stories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/df_stories.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# df_full.head()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_pickle\u001b[0;34m(self, path, compression, protocol)\u001b[0m\n\u001b[1;32m   2723\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2725\u001b[0;31m         \u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2727\u001b[0m     def to_clipboard(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mto_pickle\u001b[0;34m(obj, filepath_or_buffer, compression, protocol)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/bs4/element.py\u001b[0m in \u001b[0;36m__getnewargs__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getnewargs__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded while getting the str of an object"
     ]
    }
   ],
   "source": [
    "df_stories.to_pickle('../data/df_stories.pkl')\n",
    "# df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5315, 25)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stories.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while pickling an object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5d071b0b4aa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_stories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded while pickling an object"
     ]
    }
   ],
   "source": [
    "pkl.dump(df_stories, open('test.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = deepcopy(df_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5315, 25)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>right_story_title</th>\n",
       "      <th>right_story_url</th>\n",
       "      <th>right_story_source</th>\n",
       "      <th>right_story_leaning</th>\n",
       "      <th>right_story_text</th>\n",
       "      <th>center_story_title</th>\n",
       "      <th>center_story_url</th>\n",
       "      <th>center_story_source</th>\n",
       "      <th>center_story_leaning</th>\n",
       "      <th>center_story_text</th>\n",
       "      <th>...</th>\n",
       "      <th>mixed_story_title</th>\n",
       "      <th>mixed_story_url</th>\n",
       "      <th>mixed_story_source</th>\n",
       "      <th>mixed_story_leaning</th>\n",
       "      <th>mixed_story_text</th>\n",
       "      <th>not rated_story_title</th>\n",
       "      <th>not rated_story_url</th>\n",
       "      <th>not rated_story_source</th>\n",
       "      <th>not rated_story_leaning</th>\n",
       "      <th>not rated_story_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COVID-19 Relief Checks To Hit Some Bank Accoun...</td>\n",
       "      <td>https://dailycaller.com/2020/12/29/covid-19-re...</td>\n",
       "      <td>The Daily Caller</td>\n",
       "      <td>Right</td>\n",
       "      <td>The U.S. Treasury Secretary Steve Mnuchin said...</td>\n",
       "      <td>You could receive your $600 stimulus check as ...</td>\n",
       "      <td>https://www.usatoday.com/story/money/2020/12/2...</td>\n",
       "      <td>USA TODAY</td>\n",
       "      <td>Center</td>\n",
       "      <td>Stimulus money could be coming to your account...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Red states prioritize elderly and vulnerable i...</td>\n",
       "      <td>https://www.washingtonexaminer.com/news/red-st...</td>\n",
       "      <td>Washington Examiner</td>\n",
       "      <td>Lean Right</td>\n",
       "      <td>Several red states are eschewing federal recom...</td>\n",
       "      <td>First patient to receive COVID-19 vaccine gets...</td>\n",
       "      <td>https://thehill.com/news-by-subject/healthcare...</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>Center</td>\n",
       "      <td>Margaret Keenan, the first person outside of t...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trump Dethrones Obama for 'Most Admired Man' i...</td>\n",
       "      <td>https://townhall.com/tipsheet/cortneyobrien/20...</td>\n",
       "      <td>Townhall</td>\n",
       "      <td>Right</td>\n",
       "      <td>Former President Obama's 12-year reign is over...</td>\n",
       "      <td>Donald Trump, Michelle Obama Most Admired in 2020</td>\n",
       "      <td>https://news.gallup.com/poll/328193/donald-tru...</td>\n",
       "      <td>Gallup</td>\n",
       "      <td>Center</td>\n",
       "      <td>DATA\\nAmericans are most likely to name Presid...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Marco Rubio backs $2,000 stimulus checks, call...</td>\n",
       "      <td>https://www.foxnews.com/politics/marco-rubio-2...</td>\n",
       "      <td>Fox News (Online News)</td>\n",
       "      <td>Lean Right</td>\n",
       "      <td>Republican Florida Sen. Marco Rubio on Monday ...</td>\n",
       "      <td>U.S. Senate wrangles over $2,000 stimulus chec...</td>\n",
       "      <td>https://www.reuters.com/article/us-usa-trump/u...</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>Center</td>\n",
       "      <td>The U.S. Senate will grapple on Tuesday with w...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New York Times accused of 'glorifying' cancel ...</td>\n",
       "      <td>https://www.foxnews.com/media/new-york-times-c...</td>\n",
       "      <td>Fox News (Online News)</td>\n",
       "      <td>Lean Right</td>\n",
       "      <td>The New York Times was accused on Sunday of \"g...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   right_story_title  \\\n",
       "0  COVID-19 Relief Checks To Hit Some Bank Accoun...   \n",
       "1  Red states prioritize elderly and vulnerable i...   \n",
       "2  Trump Dethrones Obama for 'Most Admired Man' i...   \n",
       "3  Marco Rubio backs $2,000 stimulus checks, call...   \n",
       "4  New York Times accused of 'glorifying' cancel ...   \n",
       "\n",
       "                                     right_story_url      right_story_source  \\\n",
       "0  https://dailycaller.com/2020/12/29/covid-19-re...        The Daily Caller   \n",
       "1  https://www.washingtonexaminer.com/news/red-st...     Washington Examiner   \n",
       "2  https://townhall.com/tipsheet/cortneyobrien/20...                Townhall   \n",
       "3  https://www.foxnews.com/politics/marco-rubio-2...  Fox News (Online News)   \n",
       "4  https://www.foxnews.com/media/new-york-times-c...  Fox News (Online News)   \n",
       "\n",
       "  right_story_leaning                                   right_story_text  \\\n",
       "0               Right  The U.S. Treasury Secretary Steve Mnuchin said...   \n",
       "1          Lean Right  Several red states are eschewing federal recom...   \n",
       "2               Right  Former President Obama's 12-year reign is over...   \n",
       "3          Lean Right  Republican Florida Sen. Marco Rubio on Monday ...   \n",
       "4          Lean Right  The New York Times was accused on Sunday of \"g...   \n",
       "\n",
       "                                  center_story_title  \\\n",
       "0  You could receive your $600 stimulus check as ...   \n",
       "1  First patient to receive COVID-19 vaccine gets...   \n",
       "2  Donald Trump, Michelle Obama Most Admired in 2020   \n",
       "3  U.S. Senate wrangles over $2,000 stimulus chec...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                    center_story_url center_story_source  \\\n",
       "0  https://www.usatoday.com/story/money/2020/12/2...           USA TODAY   \n",
       "1  https://thehill.com/news-by-subject/healthcare...            The Hill   \n",
       "2  https://news.gallup.com/poll/328193/donald-tru...              Gallup   \n",
       "3  https://www.reuters.com/article/us-usa-trump/u...             Reuters   \n",
       "4                                                NaN                 NaN   \n",
       "\n",
       "  center_story_leaning                                  center_story_text  \\\n",
       "0               Center  Stimulus money could be coming to your account...   \n",
       "1               Center  Margaret Keenan, the first person outside of t...   \n",
       "2               Center  DATA\\nAmericans are most likely to name Presid...   \n",
       "3               Center  The U.S. Senate will grapple on Tuesday with w...   \n",
       "4                  NaN                                                NaN   \n",
       "\n",
       "   ... mixed_story_title mixed_story_url mixed_story_source  \\\n",
       "0  ...               NaN             NaN                NaN   \n",
       "1  ...               NaN             NaN                NaN   \n",
       "2  ...               NaN             NaN                NaN   \n",
       "3  ...               NaN             NaN                NaN   \n",
       "4  ...               NaN             NaN                NaN   \n",
       "\n",
       "  mixed_story_leaning mixed_story_text not rated_story_title  \\\n",
       "0                 NaN              NaN                   NaN   \n",
       "1                 NaN              NaN                   NaN   \n",
       "2                 NaN              NaN                   NaN   \n",
       "3                 NaN              NaN                   NaN   \n",
       "4                 NaN              NaN                   NaN   \n",
       "\n",
       "  not rated_story_url not rated_story_source not rated_story_leaning  \\\n",
       "0                 NaN                    NaN                     NaN   \n",
       "1                 NaN                    NaN                     NaN   \n",
       "2                 NaN                    NaN                     NaN   \n",
       "3                 NaN                    NaN                     NaN   \n",
       "4                 NaN                    NaN                     NaN   \n",
       "\n",
       "  not rated_story_text  \n",
       "0                  NaN  \n",
       "1                  NaN  \n",
       "2                  NaN  \n",
       "3                  NaN  \n",
       "4                  NaN  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in dd:\n",
    "    dd[k] = dd[k].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.to_pickle('../data/df_stories.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
